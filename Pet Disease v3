{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1122723,"sourceType":"datasetVersion","datasetId":630856},{"sourceId":9498291,"sourceType":"datasetVersion","datasetId":5421010},{"sourceId":10059275,"sourceType":"datasetVersion","datasetId":6161233},{"sourceId":10939109,"sourceType":"datasetVersion","datasetId":6802829},{"sourceId":11490697,"sourceType":"datasetVersion","datasetId":7185513}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport timm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:30:53.11769Z","iopub.execute_input":"2026-01-28T12:30:53.118486Z","iopub.status.idle":"2026-01-28T12:31:07.540796Z","shell.execute_reply.started":"2026-01-28T12:30:53.118458Z","shell.execute_reply":"2026-01-28T12:31:07.539982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup Device (GPU is available) ","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:07.542063Z","iopub.execute_input":"2026-01-28T12:31:07.542312Z","iopub.status.idle":"2026-01-28T12:31:07.606553Z","shell.execute_reply.started":"2026-01-28T12:31:07.542287Z","shell.execute_reply":"2026-01-28T12:31:07.605758Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Define Transformers (preprocessing + augmentation)","metadata":{}},{"cell_type":"code","source":"image_size = 224  # ViT models typically require 224x224 images\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:07.607707Z","iopub.execute_input":"2026-01-28T12:31:07.608011Z","iopub.status.idle":"2026-01-28T12:31:07.671826Z","shell.execute_reply.started":"2026-01-28T12:31:07.607976Z","shell.execute_reply":"2026-01-28T12:31:07.671152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Folder Structure","metadata":{}},{"cell_type":"code","source":"import os\n\n# Path to your main dataset folder on Kaggle\ndataset_path = \"/kaggle/input/pet-disease-images/data\"  # replace with your dataset path\n\n# Function to print folder structure (folders only)\ndef print_folders_only(path, indent=0):\n    for item in os.listdir(path):\n        item_path = os.path.join(path, item)\n        if os.path.isdir(item_path):\n            print(\"  \" * indent + f\"[DIR] {item}\")\n            print_folders_only(item_path, indent + 1)\n\nprint(\"Folders in your dataset:\")\nprint_folders_only(dataset_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:07.673468Z","iopub.execute_input":"2026-01-28T12:31:07.673674Z","iopub.status.idle":"2026-01-28T12:31:09.573582Z","shell.execute_reply.started":"2026-01-28T12:31:07.673654Z","shell.execute_reply":"2026-01-28T12:31:09.573063Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Separating Dataset Cats vs Dogs","metadata":{}},{"cell_type":"code","source":"import os\nimport shutil\n\n# Path to your main dataset folder\ndataset_path = \"/kaggle/input/pet-disease-images/data\"  # replace with your dataset path\n\n# Path to save the new organized folders\norganized_path = \"/kaggle/working/pet_dataset\"  # Kaggle working directory\n\n# Create main folders for Cats and Dogs\ncats_path = os.path.join(organized_path, \"Cats\")\ndogs_path = os.path.join(organized_path, \"Dogs\")\nos.makedirs(cats_path, exist_ok=True)\nos.makedirs(dogs_path, exist_ok=True)\n\n# Lists of cat and dog disease folders\ncat_folders = [\n    \"Fungal Infection in Cat\", \"Urinary Tract Infection in Cat\", \"Eye Infection in Cat\",\n    \"Skin Allergy in Cat\", \"Worm Infection in Cat\", \"Dental Disease in Cat\",\n    \"Ear Mites in Cat\", \"Feline Leukemia\", \"Feline Panleukopenia\",\n    \"Ringworm in Cat\", \"Scabies in Cat\"\n]\n\ndog_folders = [\n    \"Worm Infection in Dog\", \"Mange in Dog\", \"Distemper in Dog\", \n    \"Skin Allergy in Dog\", \"Parvovirus in Dog\", \"Tick Infestation in Dog\",\n    \"Hot Spots in Dog\", \"Fungal Infection in Dog\", \"Eye Infection in Dog\",\n    \"Kennel Cough in Dog\", \"Dental Disease in Dog\"\n]\n\n# Move folders into Cats and Dogs\nfor folder in os.listdir(dataset_path):\n    src = os.path.join(dataset_path, folder)\n    if folder in cat_folders:\n        dst = os.path.join(cats_path, folder)\n        shutil.copytree(src, dst)\n    elif folder in dog_folders:\n        dst = os.path.join(dogs_path, folder)\n        shutil.copytree(src, dst)\n\nprint(\"Folders successfully organized into Cats and Dogs!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:09.574379Z","iopub.execute_input":"2026-01-28T12:31:09.574638Z","iopub.status.idle":"2026-01-28T12:31:20.551455Z","shell.execute_reply.started":"2026-01-28T12:31:09.574604Z","shell.execute_reply":"2026-01-28T12:31:20.550856Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Counting Images Per Folder","metadata":{}},{"cell_type":"code","source":"import os\n\n# Path to your organized dataset in working directory\norganized_path = \"/kaggle/working/pet_dataset\"\n\n# Function to count images per folder\ndef count_images_per_folder(path):\n    for main_folder in os.listdir(path):\n        main_folder_path = os.path.join(path, main_folder)\n        if os.path.isdir(main_folder_path):\n            print(f\"\\n[{main_folder}]\")\n            for disease_folder in os.listdir(main_folder_path):\n                disease_path = os.path.join(main_folder_path, disease_folder)\n                if os.path.isdir(disease_path):\n                    num_images = len([f for f in os.listdir(disease_path) \n                                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n                    print(f\"  {disease_folder}: {num_images} images\")\n\ncount_images_per_folder(organized_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:20.552363Z","iopub.execute_input":"2026-01-28T12:31:20.552594Z","iopub.status.idle":"2026-01-28T12:31:20.561492Z","shell.execute_reply.started":"2026-01-28T12:31:20.552572Z","shell.execute_reply":"2026-01-28T12:31:20.560779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis) ","metadata":{}},{"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Path to organized dataset\norganized_path = \"/kaggle/working/pet_dataset\"\n\n# Function to get image counts\ndef get_image_counts(path):\n    data = []\n    for main_folder in os.listdir(path):\n        main_folder_path = os.path.join(path, main_folder)\n        if os.path.isdir(main_folder_path):\n            for disease_folder in os.listdir(main_folder_path):\n                disease_path = os.path.join(main_folder_path, disease_folder)\n                if os.path.isdir(disease_path):\n                    num_images = len([f for f in os.listdir(disease_path)\n                                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n                    data.append({\n                        \"Category\": main_folder,  # Cats or Dogs\n                        \"Disease\": disease_folder,\n                        \"Count\": num_images\n                    })\n    return data\n\n# Get data\nimage_data = get_image_counts(organized_path)\n\n# Convert to DataFrame\nimport pandas as pd\ndf = pd.DataFrame(image_data)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:20.562389Z","iopub.execute_input":"2026-01-28T12:31:20.562684Z","iopub.status.idle":"2026-01-28T12:31:21.75345Z","shell.execute_reply.started":"2026-01-28T12:31:20.562663Z","shell.execute_reply":"2026-01-28T12:31:21.752665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot Numbers of Images Per Disease","metadata":{}},{"cell_type":"code","source":"# Plot number of images per disease\nplt.figure(figsize=(15,6))\nsns.barplot(x=\"Count\", y=\"Disease\", hue=\"Category\", data=df, dodge=False)\nplt.title(\"Number of Images per Disease\")\nplt.xlabel(\"Number of Images\")\nplt.ylabel(\"Disease\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:21.754547Z","iopub.execute_input":"2026-01-28T12:31:21.754975Z","iopub.status.idle":"2026-01-28T12:31:22.165184Z","shell.execute_reply.started":"2026-01-28T12:31:21.754948Z","shell.execute_reply":"2026-01-28T12:31:22.164534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Class Distribution for Cats vs Dogs","metadata":{}},{"cell_type":"code","source":"\nplt.figure(figsize=(8,5))\nsns.countplot(x=\"Category\", data=df)\nplt.title(\"Number of Disease Classes: Cats vs Dogs\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:22.166011Z","iopub.execute_input":"2026-01-28T12:31:22.166286Z","iopub.status.idle":"2026-01-28T12:31:22.271384Z","shell.execute_reply.started":"2026-01-28T12:31:22.166263Z","shell.execute_reply":"2026-01-28T12:31:22.270773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Splitting (Train / Validation / Test) ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport shutil\n\n# Set random seed for reproducibility\nrandom.seed(42)\n\n# Paths\nsource_path = \"/kaggle/working/pet_dataset\"\nsplit_path = \"/kaggle/working/pet_dataset_split\"\n\n# Create train, val, test folders\nfor category in [\"Cats\", \"Dogs\"]:\n    for split in [\"train\", \"val\", \"test\"]:\n        os.makedirs(os.path.join(split_path, split, category), exist_ok=True)\n\n# Function to split each disease folder\ndef split_dataset(category):\n    category_path = os.path.join(source_path, category)\n    for disease_folder in os.listdir(category_path):\n        disease_path = os.path.join(category_path, disease_folder)\n        images = [f for f in os.listdir(disease_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n        random.shuffle(images)\n        \n        n_total = len(images)\n        n_train = int(0.7 * n_total)\n        n_val = int(0.15 * n_total)\n        n_test = n_total - n_train - n_val\n\n        splits = {\n            \"train\": images[:n_train],\n            \"val\": images[n_train:n_train+n_val],\n            \"test\": images[n_train+n_val:]\n        }\n\n        # Copy images to respective folders\n        for split, split_images in splits.items():\n            dest_folder = os.path.join(split_path, split, category, disease_folder)\n            os.makedirs(dest_folder, exist_ok=True)\n            for img in split_images:\n                shutil.copy(os.path.join(disease_path, img), os.path.join(dest_folder, img))\n\n# Split Cats and Dogs\nsplit_dataset(\"Cats\")\nsplit_dataset(\"Dogs\")\n\nprint(\"Dataset successfully split into train, val, and test!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:22.273724Z","iopub.execute_input":"2026-01-28T12:31:22.274154Z","iopub.status.idle":"2026-01-28T12:31:22.6855Z","shell.execute_reply.started":"2026-01-28T12:31:22.274126Z","shell.execute_reply":"2026-01-28T12:31:22.684706Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Dataset with Pytorch","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Path to split dataset\nsplit_path = \"/kaggle/working/pet_dataset_split\"\n\n# Image size for Vision Transformer (ViT expects 224x224)\nimage_size = 224\n\n# Transformations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\nval_test_transforms = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\n\n# Load datasets using ImageFolder\ntrain_dataset = datasets.ImageFolder(root=os.path.join(split_path, \"train\"), transform=train_transforms)\nval_dataset   = datasets.ImageFolder(root=os.path.join(split_path, \"val\"), transform=val_test_transforms)\ntest_dataset  = datasets.ImageFolder(root=os.path.join(split_path, \"test\"), transform=val_test_transforms)\n\n# Create DataLoaders\nbatch_size = 16\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Check dataset info\nprint(f\"Number of classes: {len(train_dataset.classes)}\")\nprint(f\"Classes: {train_dataset.classes}\")\nprint(f\"Number of training images: {len(train_dataset)}\")\nprint(f\"Number of validation images: {len(val_dataset)}\")\nprint(f\"Number of test images: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:22.686507Z","iopub.execute_input":"2026-01-28T12:31:22.686903Z","iopub.status.idle":"2026-01-28T12:31:22.701762Z","shell.execute_reply.started":"2026-01-28T12:31:22.686872Z","shell.execute_reply":"2026-01-28T12:31:22.701147Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Building Visiong Transformer","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Vision Transformer from scratch\nclass PatchEmbedding(nn.Module):\n    def __init__(self, in_channels=3, patch_size=16, embed_dim=768, img_size=224):\n        super().__init__()\n        self.patch_size = patch_size\n        self.num_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)\n        x = x.flatten(2)  # (B, embed_dim, num_patches)\n        x = x.transpose(1, 2)  # (B, num_patches, embed_dim)\n        return x\n\nclass TransformerEncoderBlock(nn.Module):\n    def __init__(self, embed_dim=768, num_heads=8, mlp_dim=2048, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, mlp_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(mlp_dim, embed_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):\n        # Self-attention\n        x2 = self.norm1(x)\n        x2 = x2.transpose(0, 1)  # MultiheadAttention expects (seq_len, batch, embed_dim)\n        attn_out, _ = self.attn(x2, x2, x2)\n        x = x + attn_out.transpose(0, 1)\n        # MLP\n        x = x + self.mlp(self.norm2(x))\n        return x\n\nclass ViT(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, in_channels=3, num_classes=22, \n                 embed_dim=768, depth=12, num_heads=8, mlp_dim=2048, dropout=0.1):\n        super().__init__()\n        self.patch_embed = PatchEmbedding(in_channels, patch_size, embed_dim, img_size)\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(torch.zeros(1, self.patch_embed.num_patches + 1, embed_dim))\n        self.dropout = nn.Dropout(dropout)\n        \n        # Transformer encoder blocks\n        self.blocks = nn.ModuleList([\n            TransformerEncoderBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(depth)\n        ])\n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        B = x.size(0)\n        x = self.patch_embed(x)\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        x = self.dropout(x)\n\n        for block in self.blocks:\n            x = block(x)\n\n        x = self.norm(x)\n        cls_output = x[:, 0]  # take the class token\n        out = self.head(cls_output)\n        return out\n\n# Example: initialize model\nnum_classes = len(train_dataset.classes)\nmodel = ViT(num_classes=num_classes).to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:22.702602Z","iopub.execute_input":"2026-01-28T12:31:22.703446Z","iopub.status.idle":"2026-01-28T12:31:23.468921Z","shell.execute_reply.started":"2026-01-28T12:31:22.703411Z","shell.execute_reply":"2026-01-28T12:31:23.468178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training ViT from Scratch","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:23.4699Z","iopub.execute_input":"2026-01-28T12:31:23.470224Z","iopub.status.idle":"2026-01-28T12:31:23.474166Z","shell.execute_reply.started":"2026-01-28T12:31:23.470186Z","shell.execute_reply":"2026-01-28T12:31:23.473593Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# assuming num_classes is already defined from train_dataset.classes\nmodel = ViT(num_classes=num_classes).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:23.475009Z","iopub.execute_input":"2026-01-28T12:31:23.475307Z","iopub.status.idle":"2026-01-28T12:31:24.050735Z","shell.execute_reply.started":"2026-01-28T12:31:23.475255Z","shell.execute_reply":"2026-01-28T12:31:24.05016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\nnum_epochs = 20\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:24.051576Z","iopub.execute_input":"2026-01-28T12:31:24.051843Z","iopub.status.idle":"2026-01-28T12:31:24.056334Z","shell.execute_reply.started":"2026-01-28T12:31:24.05182Z","shell.execute_reply":"2026-01-28T12:31:24.055612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    train_acc = 0\n    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        train_acc += (outputs.argmax(1) == labels).float().mean().item()\n    \n    train_loss /= len(train_loader)\n    train_acc /= len(train_loader)\n    \n    model.eval()\n    val_loss = 0\n    val_acc = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            val_acc += (outputs.argmax(1) == labels).float().mean().item()\n    \n    val_loss /= len(val_loader)\n    val_acc /= len(val_loader)\n    \n    scheduler.step(val_acc)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n    print(f\"Learning rate after epoch {epoch+1}: {optimizer.param_groups[0]['lr']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:31:24.057299Z","iopub.execute_input":"2026-01-28T12:31:24.057585Z","iopub.status.idle":"2026-01-28T12:46:05.171985Z","shell.execute_reply.started":"2026-01-28T12:31:24.057511Z","shell.execute_reply":"2026-01-28T12:46:05.171272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Set Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\nmodel.eval()\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Classification report\nprint(\"Classification Report:\\n\")\nprint(classification_report(all_labels, all_preds, target_names=train_dataset.classes))\n\n# Confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\nprint(\"Confusion Matrix:\\n\")\nprint(cm)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:46:05.173019Z","iopub.execute_input":"2026-01-28T12:46:05.173278Z","iopub.status.idle":"2026-01-28T12:46:11.620277Z","shell.execute_reply.started":"2026-01-28T12:46:05.173255Z","shell.execute_reply":"2026-01-28T12:46:11.619604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualization Confusion Matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', xticklabels=train_dataset.classes, yticklabels=train_dataset.classes, cmap='Blues')\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:46:11.621176Z","iopub.execute_input":"2026-01-28T12:46:11.621566Z","iopub.status.idle":"2026-01-28T12:46:11.73925Z","shell.execute_reply.started":"2026-01-28T12:46:11.621542Z","shell.execute_reply":"2026-01-28T12:46:11.738628Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Trained Model","metadata":{}},{"cell_type":"code","source":"# Save model\ntorch.save(model.state_dict(), \"/kaggle/working/vit_pet_disease.pth\")\nprint(\"Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:46:11.740161Z","iopub.execute_input":"2026-01-28T12:46:11.74038Z","iopub.status.idle":"2026-01-28T12:46:12.029382Z","shell.execute_reply.started":"2026-01-28T12:46:11.740359Z","shell.execute_reply":"2026-01-28T12:46:12.028793Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model for Interface","metadata":{}},{"cell_type":"code","source":"# Load model\nmodel = ViT(num_classes=num_classes).to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/vit_pet_disease.pth\"))\nmodel.eval()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:46:12.030305Z","iopub.execute_input":"2026-01-28T12:46:12.030537Z","iopub.status.idle":"2026-01-28T12:46:12.759996Z","shell.execute_reply.started":"2026-01-28T12:46:12.030515Z","shell.execute_reply":"2026-01-28T12:46:12.759377Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prediction Function","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\n\n# Define same transforms as validation\ntransform = transforms.Compose([\n    transforms.Resize((224,224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\n\ndef predict_image(img_path):\n    image = Image.open(img_path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(image)\n        pred = output.argmax(1).item()\n    return train_dataset.classes[pred]\n\n# Predict\nprint(\"Predicted Disease:\", predict_image(img_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:13.179513Z","iopub.execute_input":"2026-01-28T12:48:13.179863Z","iopub.status.idle":"2026-01-28T12:48:13.255621Z","shell.execute_reply.started":"2026-01-28T12:48:13.179834Z","shell.execute_reply":"2026-01-28T12:48:13.254949Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Default Query Image","metadata":{}},{"cell_type":"code","source":"# Pick an image from your organized dataset\nimg_path = \"/kaggle/working/pet_dataset/Cats/Dental Disease in Cat/Image_10.jpg\"  # replace with actual file\nprint(\"Predicted Disease:\", predict_image(img_path))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:24.429126Z","iopub.execute_input":"2026-01-28T12:48:24.429791Z","iopub.status.idle":"2026-01-28T12:48:24.502488Z","shell.execute_reply.started":"2026-01-28T12:48:24.429751Z","shell.execute_reply":"2026-01-28T12:48:24.501698Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test Case (Detect Image Path Exists) ","metadata":{}},{"cell_type":"code","source":"import os\n\nif os.path.exists(img_path):\n    print(\"Predicted Disease:\", predict_image(img_path))\nelse:\n    print(f\"File not found: {img_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:27.845273Z","iopub.execute_input":"2026-01-28T12:48:27.845573Z","iopub.status.idle":"2026-01-28T12:48:27.91758Z","shell.execute_reply.started":"2026-01-28T12:48:27.845548Z","shell.execute_reply":"2026-01-28T12:48:27.916799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimage = Image.open(img_path).convert(\"RGB\")\nplt.imshow(image)\nplt.title(\"Predicted: \" + predict_image(img_path))\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:35.986801Z","iopub.execute_input":"2026-01-28T12:48:35.987544Z","iopub.status.idle":"2026-01-28T12:48:36.44727Z","shell.execute_reply.started":"2026-01-28T12:48:35.987512Z","shell.execute_reply":"2026-01-28T12:48:36.446653Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Disease Description Information","metadata":{}},{"cell_type":"code","source":"disease_info = {\n    # Cats\n    \"Skin Allergy in Cat\": \"A skin reaction in cats causing itching, redness, and hair loss.\",\n    \"Dental Disease in Cat\": \"Gum disease or tooth problems causing pain and difficulty eating.\",\n    \"Eye Infection in Cat\": \"Infections in the eye causing redness, discharge, or swelling.\",\n    \"Worm Infection in Cat\": \"Parasitic worms in the digestive tract causing weight loss and discomfort.\",\n    \"Feline Leukemia\": \"A viral infection that weakens the immune system and may cause cancer.\",\n    \"Ringworm in Cat\": \"A fungal infection causing circular hairless patches on skin.\",\n    \"Urinary Tract Infection in Cat\": \"Infections in the bladder or urinary tract causing pain or urination issues.\",\n    \"Feline Panleukopenia\": \"A viral disease causing vomiting, diarrhea, and low white blood cell count.\",\n    \"Fungal Infection in Cat\": \"Skin or systemic fungal infections causing itching, redness, or hair loss.\",\n    \"Ear Mites in Cat\": \"Tiny parasites in the ear causing itching and dark discharge.\",\n    \"Scabies in Cat\": \"Skin infestation by mites causing severe itching and crusty skin.\",\n\n    # Dogs\n    \"Skin Allergy in Dog\": \"Skin reaction causing itching, redness, and hair loss.\",\n    \"Hot Spots in Dog\": \"Inflamed, infected areas of skin causing pain and hair loss.\",\n    \"Parvovirus in Dog\": \"Highly contagious virus causing severe vomiting and diarrhea.\",\n    \"Dental Disease in Dog\": \"Gum and tooth problems causing pain and difficulty eating.\",\n    \"Mange in Dog\": \"Skin infestation by mites causing hair loss and itching.\",\n    \"Distemper in Dog\": \"Viral infection affecting respiratory, gastrointestinal, and nervous systems.\",\n    \"Kennel Cough in Dog\": \"Contagious respiratory infection causing coughing and nasal discharge.\",\n    \"Tick Infestation in Dog\": \"Ticks attached to skin, can transmit diseases and cause irritation.\",\n    \"Fungal Infection in Dog\": \"Skin or systemic fungal infections causing itching, redness, or hair loss.\",\n    \"Worm Infection in Dog\": \"Parasitic worms causing weight loss, vomiting, and digestive issues.\",\n    \"Eye Infection in Dog\": \"Infections in the eye causing redness, discharge, or swelling.\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:40.92946Z","iopub.execute_input":"2026-01-28T12:48:40.92977Z","iopub.status.idle":"2026-01-28T12:48:40.934758Z","shell.execute_reply.started":"2026-01-28T12:48:40.929743Z","shell.execute_reply":"2026-01-28T12:48:40.934132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Updating the Prediction Function","metadata":{}},{"cell_type":"code","source":"def predict_image_with_description(img_path):\n    image = Image.open(img_path).convert(\"RGB\")\n    image = transform(image).unsqueeze(0).to(device)\n    with torch.no_grad():\n        output = model(image)\n        pred_idx = output.argmax(1).item()\n    pred_class = train_dataset.classes[pred_idx]\n    pred_description = disease_info.get(pred_class, \"No description available\")\n    return pred_class, pred_description\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:44.105908Z","iopub.execute_input":"2026-01-28T12:48:44.10621Z","iopub.status.idle":"2026-01-28T12:48:44.11128Z","shell.execute_reply.started":"2026-01-28T12:48:44.106181Z","shell.execute_reply":"2026-01-28T12:48:44.110416Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Run Prediction","metadata":{}},{"cell_type":"markdown","source":"### Query Image Path","metadata":{}},{"cell_type":"code","source":"# Example image from test set\nimg_path = \"/kaggle/working/pet_dataset/Cats/Eye Infection in Cat/Image_12.jpg\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:47.160503Z","iopub.execute_input":"2026-01-28T12:48:47.160833Z","iopub.status.idle":"2026-01-28T12:48:47.164387Z","shell.execute_reply.started":"2026-01-28T12:48:47.160804Z","shell.execute_reply":"2026-01-28T12:48:47.163676Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\npred_class, pred_desc = predict_image_with_description(img_path)\nprint(f\"Predicted Disease: {pred_class}\")\nprint(f\"Disease Info: {pred_desc}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:49.122698Z","iopub.execute_input":"2026-01-28T12:48:49.123261Z","iopub.status.idle":"2026-01-28T12:48:49.151607Z","shell.execute_reply.started":"2026-01-28T12:48:49.123232Z","shell.execute_reply":"2026-01-28T12:48:49.150959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimage = Image.open(img_path).convert(\"RGB\")\nplt.imshow(image)\nplt.title(f\"{pred_class}\\n{pred_desc}\")\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T12:48:51.404023Z","iopub.execute_input":"2026-01-28T12:48:51.404336Z","iopub.status.idle":"2026-01-28T12:48:51.549109Z","shell.execute_reply.started":"2026-01-28T12:48:51.4043Z","shell.execute_reply":"2026-01-28T12:48:51.548457Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code Improvement 1","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:57.030501Z","iopub.execute_input":"2026-01-28T13:38:57.030873Z","iopub.status.idle":"2026-01-28T13:38:57.035852Z","shell.execute_reply.started":"2026-01-28T13:38:57.030824Z","shell.execute_reply":"2026-01-28T13:38:57.035248Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CatDogDataset(Dataset):\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.image_paths = []\n        self.labels = []\n\n        for animal in [\"Cats\", \"Dogs\"]:\n            animal_path = os.path.join(root_dir, animal)\n            label = 0 if animal == \"Cats\" else 1\n\n            for disease_folder in os.listdir(animal_path):\n                disease_path = os.path.join(animal_path, disease_folder)\n                if not os.path.isdir(disease_path):\n                    continue\n\n                for img_name in os.listdir(disease_path):\n                    img_path = os.path.join(disease_path, img_name)\n                    self.image_paths.append(img_path)\n                    self.labels.append(label)\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        # ðŸ”¥ FIX: handle palette & transparency\n        image = Image.open(img_path)\n        image = image.convert(\"RGBA\").convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:38:58.7233Z","iopub.execute_input":"2026-01-28T13:38:58.72359Z","iopub.status.idle":"2026-01-28T13:38:58.731086Z","shell.execute_reply.started":"2026-01-28T13:38:58.723564Z","shell.execute_reply":"2026-01-28T13:38:58.730506Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.5, 0.5, 0.5],\n                         [0.5, 0.5, 0.5])\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:39:09.420104Z","iopub.execute_input":"2026-01-28T13:39:09.420883Z","iopub.status.idle":"2026-01-28T13:39:09.424924Z","shell.execute_reply.started":"2026-01-28T13:39:09.420853Z","shell.execute_reply":"2026-01-28T13:39:09.424307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_DIR = \"/kaggle/working/pet_dataset_split/train\"\nVALID_DIR = \"/kaggle/working/pet_dataset_split/val\"\n\ntrain_dataset = CatDogDataset(\n    root_dir=TRAIN_DIR,\n    transform=train_transforms\n)\n\nprint(\"Total training images:\", len(train_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:39:35.864566Z","iopub.execute_input":"2026-01-28T13:39:35.865158Z","iopub.status.idle":"2026-01-28T13:39:35.872025Z","shell.execute_reply.started":"2026-01-28T13:39:35.865128Z","shell.execute_reply":"2026-01-28T13:39:35.871458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img, label = train_dataset[0]\n\nprint(\"Image shape:\", img.shape)\nprint(\"Min pixel value:\", img.min().item())\nprint(\"Max pixel value:\", img.max().item())\nprint(\"Label:\", label)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:40:04.339991Z","iopub.execute_input":"2026-01-28T13:40:04.340644Z","iopub.status.idle":"2026-01-28T13:40:04.41354Z","shell.execute_reply.started":"2026-01-28T13:40:04.340614Z","shell.execute_reply":"2026-01-28T13:40:04.412869Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:40:45.154099Z","iopub.execute_input":"2026-01-28T13:40:45.154404Z","iopub.status.idle":"2026-01-28T13:40:45.159196Z","shell.execute_reply.started":"2026-01-28T13:40:45.154377Z","shell.execute_reply":"2026-01-28T13:40:45.158507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MiniViT(nn.Module):\n    def __init__(self, img_size=224, patch_size=16, num_classes=2):\n        super().__init__()\n\n        dim = 128\n        depth = 4\n        heads = 4\n\n        num_patches = (img_size // patch_size) ** 2\n        patch_dim = 3 * patch_size * patch_size\n\n        self.patch_embed = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=dim,\n            nhead=heads,\n            dropout=0.1,\n            batch_first=True\n        )\n\n        self.transformer = nn.TransformerEncoder(\n            encoder_layer,\n            num_layers=depth\n        )\n\n        self.norm = nn.LayerNorm(dim)\n        self.head = nn.Linear(dim, num_classes)\n\n        self.patch_size = patch_size\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        p = self.patch_size\n\n        # Patchify\n        x = x.unfold(2, p, p).unfold(3, p, p)\n        x = x.contiguous().view(B, C, -1, p, p)\n        x = x.permute(0, 2, 1, 3, 4)\n        x = x.reshape(B, -1, C * p * p)\n\n        x = self.patch_embed(x)\n\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n\n        x = self.transformer(x)\n        x = self.norm(x[:, 0])\n\n        return self.head(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:41:13.710445Z","iopub.execute_input":"2026-01-28T13:41:13.71099Z","iopub.status.idle":"2026-01-28T13:41:13.718481Z","shell.execute_reply.started":"2026-01-28T13:41:13.710963Z","shell.execute_reply":"2026-01-28T13:41:13.717898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MiniViT(num_classes=2).to(device)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:41:19.991194Z","iopub.execute_input":"2026-01-28T13:41:19.992079Z","iopub.status.idle":"2026-01-28T13:41:20.022291Z","shell.execute_reply.started":"2026-01-28T13:41:19.992039Z","shell.execute_reply":"2026-01-28T13:41:20.021698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_size=32,\n    shuffle=True,\n    num_workers=2,\n    pin_memory=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:41:26.254683Z","iopub.execute_input":"2026-01-28T13:41:26.255439Z","iopub.status.idle":"2026-01-28T13:41:26.259738Z","shell.execute_reply.started":"2026-01-28T13:41:26.255407Z","shell.execute_reply":"2026-01-28T13:41:26.259164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=1e-4,\n    weight_decay=1e-4\n)\n\nscheduler = optim.lr_scheduler.CosineAnnealingLR(\n    optimizer,\n    T_max=40\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:41:33.728355Z","iopub.execute_input":"2026-01-28T13:41:33.728666Z","iopub.status.idle":"2026-01-28T13:41:33.734217Z","shell.execute_reply.started":"2026-01-28T13:41:33.728636Z","shell.execute_reply":"2026-01-28T13:41:33.733563Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 40\n\nfor epoch in range(num_epochs):\n    model.train()\n    correct = 0\n    total = 0\n    running_loss = 0\n\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    scheduler.step()\n\n    train_acc = correct / total\n    print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Acc: {train_acc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:41:39.168828Z","iopub.execute_input":"2026-01-28T13:41:39.169413Z","iopub.status.idle":"2026-01-28T13:50:41.360509Z","shell.execute_reply.started":"2026-01-28T13:41:39.169383Z","shell.execute_reply":"2026-01-28T13:50:41.359664Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nimages, labels = next(iter(train_loader))\nimages = images.to(device)\n\nwith torch.no_grad():\n    outputs = model(images)\n\nprint(outputs.softmax(dim=1)[:5])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:51:04.151293Z","iopub.execute_input":"2026-01-28T13:51:04.151615Z","iopub.status.idle":"2026-01-28T13:51:06.429425Z","shell.execute_reply.started":"2026-01-28T13:51:04.151582Z","shell.execute_reply":"2026-01-28T13:51:06.428673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"VAL_DIR = \"/kaggle/working/pet_dataset_split/val\"\n\nval_dataset = CatDogDataset(\n    root_dir=VAL_DIR,\n    transform=train_transforms   # no heavy aug for now\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2\n)\n\nprint(\"Validation images:\", len(val_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:51:58.377398Z","iopub.execute_input":"2026-01-28T13:51:58.377688Z","iopub.status.idle":"2026-01-28T13:51:58.38433Z","shell.execute_reply.started":"2026-01-28T13:51:58.377658Z","shell.execute_reply":"2026-01-28T13:51:58.383652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 40\n\nfor epoch in range(num_epochs):\n    # ---- TRAIN ----\n    model.train()\n    train_correct, train_total = 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(1)\n        train_correct += (preds == labels).sum().item()\n        train_total += labels.size(0)\n\n    train_acc = train_correct / train_total\n\n    # ---- VALIDATION ----\n    model.eval()\n    val_correct, val_total = 0, 0\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(1)\n            val_correct += (preds == labels).sum().item()\n            val_total += labels.size(0)\n\n    val_acc = val_correct / val_total\n    scheduler.step()\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n          f\"Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T13:52:56.131439Z","iopub.execute_input":"2026-01-28T13:52:56.132107Z","iopub.status.idle":"2026-01-28T14:04:31.718619Z","shell.execute_reply.started":"2026-01-28T13:52:56.132065Z","shell.execute_reply":"2026-01-28T14:04:31.717709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\ny_true = []\ny_pred = []\n\nmodel.eval()\nwith torch.no_grad():\n    for images, labels in val_loader:\n        images = images.to(device)\n        outputs = model(images)\n        preds = outputs.argmax(1).cpu().numpy()\n        y_pred.extend(preds)\n        y_true.extend(labels.numpy())\n\nprint(\"Classification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=[\"Cat\", \"Dog\"]))\n\nprint(\"Confusion Matrix:\\n\")\nprint(confusion_matrix(y_true, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:09:22.034576Z","iopub.execute_input":"2026-01-28T14:09:22.03542Z","iopub.status.idle":"2026-01-28T14:09:25.690734Z","shell.execute_reply.started":"2026-01-28T14:09:22.035381Z","shell.execute_reply":"2026-01-28T14:09:25.690011Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code Improvement 2","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ColorJitter(\n        brightness=0.2,\n        contrast=0.2,\n        saturation=0.2,\n        hue=0.05\n    ),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:12:35.759787Z","iopub.execute_input":"2026-01-28T14:12:35.760133Z","iopub.status.idle":"2026-01-28T14:12:35.766635Z","shell.execute_reply.started":"2026-01-28T14:12:35.760096Z","shell.execute_reply":"2026-01-28T14:12:35.766089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass ImprovedCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 32, 3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.25),\n\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n            nn.Dropout(0.30),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, 256),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.classifier(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:12:45.20013Z","iopub.execute_input":"2026-01-28T14:12:45.200767Z","iopub.status.idle":"2026-01-28T14:12:45.20717Z","shell.execute_reply.started":"2026-01-28T14:12:45.200709Z","shell.execute_reply":"2026-01-28T14:12:45.206538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:12:53.56082Z","iopub.execute_input":"2026-01-28T14:12:53.561453Z","iopub.status.idle":"2026-01-28T14:12:53.565006Z","shell.execute_reply.started":"2026-01-28T14:12:53.561422Z","shell.execute_reply":"2026-01-28T14:12:53.56436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=3e-4,\n    weight_decay=1e-4\n)\n\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='max',\n    factor=0.5,\n    patience=3\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:13:01.847586Z","iopub.execute_input":"2026-01-28T14:13:01.848231Z","iopub.status.idle":"2026-01-28T14:13:01.853439Z","shell.execute_reply.started":"2026-01-28T14:13:01.848197Z","shell.execute_reply":"2026-01-28T14:13:01.852938Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=6):\n        self.patience = patience\n        self.best_score = None\n        self.counter = 0\n        self.stop = False\n\n    def __call__(self, score):\n        if self.best_score is None or score > self.best_score:\n            self.best_score = score\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.stop = True\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:13:09.266062Z","iopub.execute_input":"2026-01-28T14:13:09.266655Z","iopub.status.idle":"2026-01-28T14:13:09.271286Z","shell.execute_reply.started":"2026-01-28T14:13:09.266626Z","shell.execute_reply":"2026-01-28T14:13:09.27056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_epochs = 50\nearly_stopper = EarlyStopping(patience=7)\n\nfor epoch in range(num_epochs):\n    model.train()\n    correct, total = 0, 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        preds = outputs.argmax(1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    train_acc = correct / total\n\n    # Validation\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            preds = outputs.argmax(1)\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n\n    val_acc = correct / total\n    scheduler.step(val_acc)\n\n    print(f\"Epoch [{epoch+1}] Train Acc: {train_acc:.3f} | Val Acc: {val_acc:.3f}\")\n\n    early_stopper(val_acc)\n    if early_stopper.stop:\n        print(\"ðŸ›‘ Early stopping triggered\")\n        break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T14:13:17.760785Z","iopub.execute_input":"2026-01-28T14:13:17.761248Z","iopub.status.idle":"2026-01-28T14:15:04.158794Z","shell.execute_reply.started":"2026-01-28T14:13:17.761218Z","shell.execute_reply":"2026-01-28T14:15:04.157266Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Code Improvement 3","metadata":{}},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:05:51.555922Z","iopub.execute_input":"2026-01-28T15:05:51.556665Z","iopub.status.idle":"2026-01-28T15:05:59.634422Z","shell.execute_reply.started":"2026-01-28T15:05:51.556636Z","shell.execute_reply":"2026-01-28T15:05:59.633776Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Loading","metadata":{}},{"cell_type":"code","source":"import os\n\nprint(\"Available datasets in /kaggle/input:\\n\")\nfor d in os.listdir(\"/kaggle/input\"):\n    print(\"-\", d)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:08:41.162187Z","iopub.execute_input":"2026-01-28T15:08:41.162857Z","iopub.status.idle":"2026-01-28T15:08:41.168173Z","shell.execute_reply.started":"2026-01-28T15:08:41.162829Z","shell.execute_reply":"2026-01-28T15:08:41.167517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Labeling","metadata":{}},{"cell_type":"code","source":"LABELS = {\n    \"cat_skin\": \"cat_skin_disease\",\n    \"dog_skin\": \"dog_skin_disease\",\n    \"healthy\": \"healthy\"\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:11:23.463482Z","iopub.execute_input":"2026-01-28T15:11:23.463807Z","iopub.status.idle":"2026-01-28T15:11:23.490380Z","shell.execute_reply.started":"2026-01-28T15:11:23.463781Z","shell.execute_reply":"2026-01-28T15:11:23.489710Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CREATE MASTER METADATA DATAFRAME","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nmaster_data = []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:15:26.216606Z","iopub.execute_input":"2026-01-28T15:15:26.217222Z","iopub.status.idle":"2026-01-28T15:15:26.221744Z","shell.execute_reply.started":"2026-01-28T15:15:26.217191Z","shell.execute_reply":"2026-01-28T15:15:26.220737Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process DOG SKIN DISEASE DATASET","metadata":{}},{"cell_type":"code","source":"DOG_SKIN_PATH = \"/kaggle/input/dogs-skin-diseases-image-dataset\"\n\nfor disease in os.listdir(DOG_SKIN_PATH):\n    disease_path = os.path.join(DOG_SKIN_PATH, disease)\n    if not os.path.isdir(disease_path):\n        continue\n        \n    for img in os.listdir(disease_path):\n        master_data.append({\n            \"image_path\": os.path.join(disease_path, img),\n            \"pet\": \"dog\",\n            \"disease\": \"skin_disease\",\n            \"source\": \"dogs_skin\"\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:15:45.598225Z","iopub.execute_input":"2026-01-28T15:15:45.598994Z","iopub.status.idle":"2026-01-28T15:15:45.616155Z","shell.execute_reply.started":"2026-01-28T15:15:45.598966Z","shell.execute_reply":"2026-01-28T15:15:45.615612Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process CAT SKIN DISEASE DATASET","metadata":{}},{"cell_type":"code","source":"CAT_SKIN_PATH = \"/kaggle/input/cat-skin-disease-v2\"\n\nfor disease in os.listdir(CAT_SKIN_PATH):\n    disease_path = os.path.join(CAT_SKIN_PATH, disease)\n    if not os.path.isdir(disease_path):\n        continue\n        \n    for img in os.listdir(disease_path):\n        master_data.append({\n            \"image_path\": os.path.join(disease_path, img),\n            \"pet\": \"cat\",\n            \"disease\": \"skin_disease\",\n            \"source\": \"cat_skin\"\n        })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:16:07.185075Z","iopub.execute_input":"2026-01-28T15:16:07.185474Z","iopub.status.idle":"2026-01-28T15:16:07.196528Z","shell.execute_reply.started":"2026-01-28T15:16:07.185443Z","shell.execute_reply":"2026-01-28T15:16:07.196018Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Process PET-DISEASE-IMAGES (Mixed)","metadata":{}},{"cell_type":"code","source":"MIXED_PATH = \"/kaggle/input/pet-disease-images\"\n\nfor root, dirs, files in os.walk(MIXED_PATH):\n    for file in files:\n        if file.lower().endswith((\".jpg\", \".png\")):\n            pet = \"dog\" if \"dog\" in root.lower() else \"cat\"\n            master_data.append({\n                \"image_path\": os.path.join(root, file),\n                \"pet\": pet,\n                \"disease\": \"skin_disease\",\n                \"source\": \"mixed\"\n            })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:16:33.941099Z","iopub.execute_input":"2026-01-28T15:16:33.941448Z","iopub.status.idle":"2026-01-28T15:16:38.392063Z","shell.execute_reply.started":"2026-01-28T15:16:33.941420Z","shell.execute_reply":"2026-01-28T15:16:38.391240Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FINAL DATAFRAME & SANITY CHECK","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(master_data)\n\nprint(df.head())\nprint(\"\\nPet distribution:\\n\", df[\"pet\"].value_counts())\nprint(\"\\nSource distribution:\\n\", df[\"source\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:17:00.961432Z","iopub.execute_input":"2026-01-28T15:17:00.961709Z","iopub.status.idle":"2026-01-28T15:17:01.023319Z","shell.execute_reply.started":"2026-01-28T15:17:00.961688Z","shell.execute_reply":"2026-01-28T15:17:01.022496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clean Meta Data Mandatory","metadata":{}},{"cell_type":"code","source":"import os\n\ndef is_image_file(path):\n    return (\n        os.path.isfile(path) and \n        path.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n    )\n\ndf = df[df[\"image_path\"].apply(is_image_file)].reset_index(drop=True)\n\nprint(\"Cleaned dataset size:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:18:45.717067Z","iopub.execute_input":"2026-01-28T15:18:45.717989Z","iopub.status.idle":"2026-01-28T15:18:46.659114Z","shell.execute_reply.started":"2026-01-28T15:18:45.717958Z","shell.execute_reply":"2026-01-28T15:18:46.658442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SAFE IMAGE SIZE EDA (ROBUST VERSION)","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom tqdm import tqdm\nimport pandas as pd\n\nsizes = []\nbad_images = []\n\nfor p in tqdm(df[\"image_path\"].sample(min(300, len(df)))):\n    try:\n        with Image.open(p) as img:\n            sizes.append(img.size)\n    except Exception as e:\n        bad_images.append(p)\n\nprint(\"Bad images skipped:\", len(bad_images))\n\nsizes_df = pd.DataFrame(sizes, columns=[\"W\", \"H\"])\nsizes_df.describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:19:13.370075Z","iopub.execute_input":"2026-01-28T15:19:13.370662Z","iopub.status.idle":"2026-01-28T15:19:16.551091Z","shell.execute_reply.started":"2026-01-28T15:19:13.370633Z","shell.execute_reply":"2026-01-28T15:19:16.550562Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## REMOVE BAD IMAGES PERMANENTLY","metadata":{}},{"cell_type":"code","source":"df = df[~df[\"image_path\"].isin(bad_images)].reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:19:45.645831Z","iopub.execute_input":"2026-01-28T15:19:45.646146Z","iopub.status.idle":"2026-01-28T15:19:45.652341Z","shell.execute_reply.started":"2026-01-28T15:19:45.646119Z","shell.execute_reply":"2026-01-28T15:19:45.651586Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FINAL EDA (ON CLEANED DATA)","metadata":{}},{"cell_type":"code","source":"print(\"Total samples:\", len(df))\nprint(\"\\nPets:\\n\", df[\"pet\"].value_counts())\nprint(\"\\nDiseases:\\n\", df[\"disease\"].value_counts())\nprint(\"\\nSources:\\n\", df[\"source\"].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:20:48.742968Z","iopub.execute_input":"2026-01-28T15:20:48.743815Z","iopub.status.idle":"2026-01-28T15:20:48.751443Z","shell.execute_reply.started":"2026-01-28T15:20:48.743776Z","shell.execute_reply":"2026-01-28T15:20:48.750660Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Class Distribution","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(6,4))\nsns.countplot(data=df, x=\"pet\")\nplt.title(\"Pet Distribution\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:21:42.572189Z","iopub.execute_input":"2026-01-28T15:21:42.572544Z","iopub.status.idle":"2026-01-28T15:21:42.683383Z","shell.execute_reply.started":"2026-01-28T15:21:42.572516Z","shell.execute_reply":"2026-01-28T15:21:42.682621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,4))\nsns.countplot(data=df, x=\"disease\")\nplt.title(\"Disease Distribution\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:21:24.036401Z","iopub.execute_input":"2026-01-28T15:21:24.037188Z","iopub.status.idle":"2026-01-28T15:21:24.145522Z","shell.execute_reply.started":"2026-01-28T15:21:24.037138Z","shell.execute_reply":"2026-01-28T15:21:24.144873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(8,4))\nsns.countplot(data=df, x=\"source\")\nplt.title(\"Source Dataset Distribution\")\nplt.xticks(rotation=30)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:21:38.431308Z","iopub.execute_input":"2026-01-28T15:21:38.432064Z","iopub.status.idle":"2026-01-28T15:21:38.554086Z","shell.execute_reply.started":"2026-01-28T15:21:38.432036Z","shell.execute_reply":"2026-01-28T15:21:38.553433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visual Inspection","metadata":{}},{"cell_type":"code","source":"import random\nfrom PIL import Image\n\nsamples = df.sample(6)\n\nplt.figure(figsize=(12,6))\nfor i, (_, row) in enumerate(samples.iterrows()):\n    img = Image.open(row[\"image_path\"])\n    plt.subplot(2,3,i+1)\n    plt.imshow(img)\n    plt.title(f\"{row['pet']} | {row['disease']}\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:22:37.408002Z","iopub.execute_input":"2026-01-28T15:22:37.408599Z","iopub.status.idle":"2026-01-28T15:22:38.318044Z","shell.execute_reply.started":"2026-01-28T15:22:37.408572Z","shell.execute_reply":"2026-01-28T15:22:38.316865Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LABEL ENCODING (EXPLICIT & CLEAN)","metadata":{}},{"cell_type":"code","source":"df[\"target\"] = df[\"pet\"] + \"_\" + df[\"disease\"]\ndf[\"target\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:23:14.585085Z","iopub.execute_input":"2026-01-28T15:23:14.585745Z","iopub.status.idle":"2026-01-28T15:23:14.592861Z","shell.execute_reply.started":"2026-01-28T15:23:14.585714Z","shell.execute_reply":"2026-01-28T15:23:14.592306Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Encode Labels","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf[\"label\"] = le.fit_transform(df[\"target\"])\n\nprint(dict(zip(le.classes_, le.transform(le.classes_))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:23:39.187769Z","iopub.execute_input":"2026-01-28T15:23:39.188394Z","iopub.status.idle":"2026-01-28T15:23:39.482155Z","shell.execute_reply.started":"2026-01-28T15:23:39.188366Z","shell.execute_reply":"2026-01-28T15:23:39.481435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train Validation Split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(\n    df,\n    test_size=0.2,\n    stratify=df[\"label\"],\n    random_state=42\n)\n\nprint(\"Train size:\", len(train_df))\nprint(\"Val size:\", len(val_df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:24:20.194156Z","iopub.execute_input":"2026-01-28T15:24:20.195124Z","iopub.status.idle":"2026-01-28T15:24:20.291974Z","shell.execute_reply.started":"2026-01-28T15:24:20.195092Z","shell.execute_reply":"2026-01-28T15:24:20.291398Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Image Preprocessing ViT Friendly","metadata":{}},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\nIMG_SIZE = 224\n\nfrom torchvision import transforms\n\ntrain_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n    transforms.ToTensor()\n])\n\n\nval_tfms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor()\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:51:14.606706Z","iopub.execute_input":"2026-01-28T15:51:14.607718Z","iopub.status.idle":"2026-01-28T15:51:14.612649Z","shell.execute_reply.started":"2026-01-28T15:51:14.607684Z","shell.execute_reply":"2026-01-28T15:51:14.612025Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CUSTOM PYTORCH DATASET CLASS","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass PetDiseaseDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df.reset_index(drop=True)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        img_path = self.df.loc[idx, \"image_path\"]\n        label = self.df.loc[idx, \"label\"]\n\n        img = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:25:28.001841Z","iopub.execute_input":"2026-01-28T15:25:28.002455Z","iopub.status.idle":"2026-01-28T15:25:28.007600Z","shell.execute_reply.started":"2026-01-28T15:25:28.002428Z","shell.execute_reply":"2026-01-28T15:25:28.006756Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dataset Loader","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_ds = PetDiseaseDataset(train_df, train_tfms)\nval_ds   = PetDiseaseDataset(val_df, val_tfms)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:26:27.315242Z","iopub.execute_input":"2026-01-28T15:26:27.316004Z","iopub.status.idle":"2026-01-28T15:26:27.321385Z","shell.execute_reply.started":"2026-01-28T15:26:27.315977Z","shell.execute_reply":"2026-01-28T15:26:27.320735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Patch Embedding","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_ds = PetDiseaseDataset(train_df, train_tfms)\nval_ds   = PetDiseaseDataset(val_df, val_tfms)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader   = DataLoader(val_ds, batch_size=32, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:28:04.943515Z","iopub.execute_input":"2026-01-28T15:28:04.944240Z","iopub.status.idle":"2026-01-28T15:28:04.949356Z","shell.execute_reply.started":"2026-01-28T15:28:04.944212Z","shell.execute_reply":"2026-01-28T15:28:04.948739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nPATCH_SIZE = 16     # 16x16 patches\nIN_CHANNELS = 3\nEMBED_DIM = 256     # model dimension\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:28:15.660633Z","iopub.execute_input":"2026-01-28T15:28:15.661312Z","iopub.status.idle":"2026-01-28T15:28:15.664398Z","shell.execute_reply.started":"2026-01-28T15:28:15.661279Z","shell.execute_reply":"2026-01-28T15:28:15.663735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_PATCHES = (IMG_SIZE // PATCH_SIZE) ** 2\nprint(\"Number of patches:\", NUM_PATCHES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:28:22.759927Z","iopub.execute_input":"2026-01-28T15:28:22.760587Z","iopub.status.idle":"2026-01-28T15:28:22.764217Z","shell.execute_reply.started":"2026-01-28T15:28:22.760555Z","shell.execute_reply":"2026-01-28T15:28:22.763526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Patch Embedding Layer","metadata":{}},{"cell_type":"code","source":"class PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super().__init__()\n        self.patch_size = patch_size\n        \n        self.proj = nn.Conv2d(\n            in_channels,\n            embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size\n        )\n\n    def forward(self, x):\n        # x: [B, 3, 224, 224]\n        x = self.proj(x)          # [B, embed_dim, 14, 14]\n        x = x.flatten(2)          # [B, embed_dim, N]\n        x = x.transpose(1, 2)     # [B, N, embed_dim]\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:28:53.202241Z","iopub.execute_input":"2026-01-28T15:28:53.202870Z","iopub.status.idle":"2026-01-28T15:28:53.207284Z","shell.execute_reply.started":"2026-01-28T15:28:53.202844Z","shell.execute_reply":"2026-01-28T15:28:53.206570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ViTEmbedding(nn.Module):\n    def __init__(self, num_patches, embed_dim):\n        super().__init__()\n        \n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n        self.pos_embed = nn.Parameter(\n            torch.zeros(1, num_patches + 1, embed_dim)\n        )\n\n    def forward(self, x):\n        B = x.size(0)\n        \n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x = x + self.pos_embed\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:29:09.142096Z","iopub.execute_input":"2026-01-28T15:29:09.142903Z","iopub.status.idle":"2026-01-28T15:29:09.147506Z","shell.execute_reply.started":"2026-01-28T15:29:09.142873Z","shell.execute_reply":"2026-01-28T15:29:09.146891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadSelfAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        assert embed_dim % num_heads == 0\n        \n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        \n        qkv = self.qkv(x)                     # [B, N, 3C]\n        qkv = qkv.reshape(\n            B, N, 3, self.num_heads, self.head_dim\n        ).permute(2, 0, 3, 1, 4)\n        \n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        \n        out = (attn @ v)\n        out = out.transpose(1, 2).reshape(B, N, C)\n        \n        return self.proj(out)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:29:25.774872Z","iopub.execute_input":"2026-01-28T15:29:25.775183Z","iopub.status.idle":"2026-01-28T15:29:25.781131Z","shell.execute_reply.started":"2026-01-28T15:29:25.775157Z","shell.execute_reply":"2026-01-28T15:29:25.780531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n        super().__init__()\n        \n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n        \n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n            nn.GELU(),\n            nn.Linear(embed_dim * mlp_ratio, embed_dim)\n        )\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:29:37.852746Z","iopub.execute_input":"2026-01-28T15:29:37.853050Z","iopub.status.idle":"2026-01-28T15:29:37.858183Z","shell.execute_reply.started":"2026-01-28T15:29:37.853026Z","shell.execute_reply":"2026-01-28T15:29:37.857511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class VisionTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size,\n        patch_size,\n        in_channels,\n        num_classes,\n        embed_dim=256,\n        depth=6,\n        num_heads=8\n    ):\n        super().__init__()\n        \n        self.patch_embed = PatchEmbedding(\n            img_size, patch_size, in_channels, embed_dim\n        )\n        \n        self.embed = ViTEmbedding(NUM_PATCHES, embed_dim)\n        \n        self.blocks = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads)\n            for _ in range(depth)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        self.head = nn.Linear(embed_dim, num_classes)\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.embed(x)\n        \n        for block in self.blocks:\n            x = block(x)\n        \n        cls_token = self.norm(x[:, 0])\n        return self.head(cls_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:29:48.018100Z","iopub.execute_input":"2026-01-28T15:29:48.018490Z","iopub.status.idle":"2026-01-28T15:29:48.024400Z","shell.execute_reply.started":"2026-01-28T15:29:48.018447Z","shell.execute_reply":"2026-01-28T15:29:48.023673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"NUM_CLASSES = len(le.classes_)\n\nmodel = VisionTransformer(\n    img_size=224,\n    patch_size=16,\n    in_channels=3,\n    num_classes=NUM_CLASSES\n)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:29:59.704072Z","iopub.execute_input":"2026-01-28T15:29:59.704651Z","iopub.status.idle":"2026-01-28T15:29:59.960370Z","shell.execute_reply.started":"2026-01-28T15:29:59.704624Z","shell.execute_reply":"2026-01-28T15:29:59.959631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\ncriterion = nn.CrossEntropyLoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:30:35.389659Z","iopub.execute_input":"2026-01-28T15:30:35.390491Z","iopub.status.idle":"2026-01-28T15:30:35.394030Z","shell.execute_reply.started":"2026-01-28T15:30:35.390449Z","shell.execute_reply":"2026-01-28T15:30:35.393479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.AdamW(\n    model.parameters(),\n    lr=3e-4,\n    weight_decay=1e-4\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:30:44.229548Z","iopub.execute_input":"2026-01-28T15:30:44.230184Z","iopub.status.idle":"2026-01-28T15:30:44.234731Z","shell.execute_reply.started":"2026-01-28T15:30:44.230147Z","shell.execute_reply":"2026-01-28T15:30:44.234051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, 1)\n    return (preds == labels).float().mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:31:20.843055Z","iopub.execute_input":"2026-01-28T15:31:20.843739Z","iopub.status.idle":"2026-01-28T15:31:20.847106Z","shell.execute_reply.started":"2026-01-28T15:31:20.843710Z","shell.execute_reply":"2026-01-28T15:31:20.846567Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    \n    running_loss = 0.0\n    running_acc = 0.0\n    \n    for imgs, labels in loader:\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        running_acc += accuracy(outputs, labels).item()\n    \n    return running_loss / len(loader), running_acc / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:31:29.137786Z","iopub.execute_input":"2026-01-28T15:31:29.138075Z","iopub.status.idle":"2026-01-28T15:31:29.142815Z","shell.execute_reply.started":"2026-01-28T15:31:29.138051Z","shell.execute_reply":"2026-01-28T15:31:29.142207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def validate_one_epoch(model, loader, criterion, device):\n    model.eval()\n    \n    running_loss = 0.0\n    running_acc = 0.0\n    \n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n            \n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            running_acc += accuracy(outputs, labels).item()\n    \n    return running_loss / len(loader), running_acc / len(loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:31:40.355314Z","iopub.execute_input":"2026-01-28T15:31:40.355624Z","iopub.status.idle":"2026-01-28T15:31:40.359975Z","shell.execute_reply.started":"2026-01-28T15:31:40.355599Z","shell.execute_reply":"2026-01-28T15:31:40.359367Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"EPOCHS = 10\n\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_one_epoch(\n        model, train_loader, optimizer, criterion, device\n    )\n    \n    val_loss, val_acc = validate_one_epoch(\n        model, val_loader, criterion, device\n    )\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n    \n    print(\n        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\"\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:31:48.972864Z","iopub.execute_input":"2026-01-28T15:31:48.973578Z","iopub.status.idle":"2026-01-28T15:37:14.626781Z","shell.execute_reply.started":"2026-01-28T15:31:48.973547Z","shell.execute_reply":"2026-01-28T15:37:14.626111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.plot(val_losses, label=\"Val Loss\")\nplt.legend()\nplt.title(\"Loss Curve\")\n\nplt.subplot(1,2,2)\nplt.plot(train_accs, label=\"Train Acc\")\nplt.plot(val_accs, label=\"Val Acc\")\nplt.legend()\nplt.title(\"Accuracy Curve\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:38:55.707752Z","iopub.execute_input":"2026-01-28T15:38:55.708454Z","iopub.status.idle":"2026-01-28T15:38:55.953707Z","shell.execute_reply.started":"2026-01-28T15:38:55.708415Z","shell.execute_reply":"2026-01-28T15:38:55.953019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm import tqdm\n\n# 1ï¸âƒ£ Loss & Optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n\n# 2ï¸âƒ£ Scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# 3ï¸âƒ£ Accuracy function\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, 1)\n    return (preds == labels).float().mean()\n\n# 4ï¸âƒ£ Training / validation loops\ndef train_one_epoch(model, loader, optimizer, criterion, device):\n    model.train()\n    running_loss, running_acc = 0.0, 0.0\n    \n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        running_acc += accuracy(outputs, labels).item()\n    \n    return running_loss / len(loader), running_acc / len(loader)\n\ndef validate_one_epoch(model, loader, criterion, device):\n    model.eval()\n    running_loss, running_acc = 0.0, 0.0\n    \n    with torch.no_grad():\n        for imgs, labels in loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            outputs = model(imgs)\n            loss = criterion(outputs, labels)\n            \n            running_loss += loss.item()\n            running_acc += accuracy(outputs, labels).item()\n    \n    return running_loss / len(loader), running_acc / len(loader)\n\n# 5ï¸âƒ£ Training loop with scheduler\nbest_val_acc = 0.0\nbest_model_wts = None\n\ntrain_losses, val_losses = [], []\ntrain_accs, val_accs = [], []\n\nfor epoch in range(EPOCHS):\n    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n    val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device)\n    \n    # Step scheduler at **end of each epoch**\n    scheduler.step()\n    \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    train_accs.append(train_acc)\n    val_accs.append(val_acc)\n    \n    # Save best weights\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_model_wts = model.state_dict()\n    \n    print(\n        f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n        f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f} | \"\n        f\"LR: {scheduler.get_last_lr()[0]:.6f}\"\n    )\n\n# Load best model\nif best_model_wts is not None:\n    model.load_state_dict(best_model_wts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T15:54:23.250341Z","iopub.execute_input":"2026-01-28T15:54:23.250695Z","iopub.status.idle":"2026-01-28T15:59:30.829094Z","shell.execute_reply.started":"2026-01-28T15:54:23.250669Z","shell.execute_reply":"2026-01-28T15:59:30.828362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference Pipeline","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nimport torchvision.transforms as transforms\n\n# 1ï¸âƒ£ Define transform (same as validation)\nIMG_SIZE = 224\ntransform = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor()\n])\n\n# 2ï¸âƒ£ Load user image\ndef load_image(path):\n    img = Image.open(path).convert(\"RGB\")\n    img = transform(img)\n    img = img.unsqueeze(0)  # add batch dimension\n    return img\n\n# 3ï¸âƒ£ Predict function\ndef predict(model, image_tensor, device, label_encoder):\n    model.eval()\n    image_tensor = image_tensor.to(device)\n    \n    with torch.no_grad():\n        output = model(image_tensor)\n        pred_label = torch.argmax(output, dim=1).cpu().numpy()\n        pred_class = label_encoder.inverse_transform(pred_label)[0]\n    \n    # Robust splitting\n    parts = pred_class.split(\"_\")\n    pet = parts[0].capitalize()\n    \n    if len(parts) > 2:\n        # Join remaining parts as disease name\n        disease = \" \".join(parts[1:]).replace(\"skin disease\", \"Skin Disease\").capitalize()\n    else:\n        disease = parts[1].replace(\"skin_disease\",\"Skin Disease\").capitalize()\n    \n    return pet, disease\n\n\n# 4ï¸âƒ£ Example usage\nuser_image_path = \"/kaggle/input/dogs-skin-diseases-image-dataset/train/Fungal_infections/1016_jpg.rf.a666986980e03f0f980d3bdb19ccaab3.jpg\"  # replace with your image\nimg_tensor = load_image(user_image_path)\npet, disease = predict(model, img_tensor, device, le)\n\nprint(f\"Pet: {pet}\")\nprint(f\"Prediction: {disease}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-28T16:03:31.046603Z","iopub.execute_input":"2026-01-28T16:03:31.047455Z","iopub.status.idle":"2026-01-28T16:03:31.069180Z","shell.execute_reply.started":"2026-01-28T16:03:31.047425Z","shell.execute_reply":"2026-01-28T16:03:31.068445Z"}},"outputs":[],"execution_count":null}]}